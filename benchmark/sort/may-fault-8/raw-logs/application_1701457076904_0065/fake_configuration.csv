,name,value,description
0,io.bytes.per.checksum,-2026973605,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,dfs.datanode.readahead.bytes,4194304,"While reading block files, if the Hadoop native libraries are available,
        the datanode can use the posix_fadvise system call to explicitly
        page data into the operating system buffer cache ahead of the current
        reader's position. This can improve performance especially when
        disks are highly contended.

        This configuration specifies the number of bytes ahead of the current
        read position which the datanode will attempt to read ahead. This
        feature may be disabled by configuring this property to 0.

        If the native libraries are not available, this configuration has no
        effect."
2,dfs.client.write.exclude.nodes.cache.expiry.interval.millis,600000,"The maximum period to keep a DN in the excluded nodes list
  at a client. After this period, in milliseconds, the previously excluded node(s) will
  be removed automatically from the cache and will be considered good for block allocations
  again. Useful to lower or raise in situations where you keep a file open for very long
  periods (such as a Write-Ahead-Log (WAL) file) to make the writer tolerant to cluster maintenance
  restarts. Defaults to 10 minutes."
3,dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold,10737418240,"Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to
    org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy.
    This setting controls how much DN volumes are allowed to differ in terms of
    bytes of free disk space before they are considered imbalanced. If the free
    space of all the volumes are within this range of each other, the volumes
    will be considered balanced and block assignments will be done on a pure
    round robin basis. Support multiple size unit suffix(case insensitive), as
    described in dfs.blocksize."
4,yarn.resourcemanager.connect.retry-interval.ms,30000,
5,yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-size,500,
6,hadoop.registry.zk.retry.times,5,Zookeeper connection retry count before failing
7,mapreduce.input.lineinputformat.linespermap,1,"When using NLineInputFormat, the number of lines of input data
  to include in each split."
8,dfs.disk.balancer.max.disk.throughputInMBperSec,10,"Maximum disk bandwidth used by diskbalancer
      during read from a source disk. The unit is MB/sec."
9,yarn.resourcemanager.epoch.range,0,
