,name,value,description
0,io.bytes.per.checksum,1490559308,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,mapreduce.reduce.skip.maxgroups,0,"The number of acceptable skip groups surrounding the bad
    group PER bad group in reducer. The number includes the bad group as well.
    To turn the feature of detection/skipping of bad groups off, set the
    value to 0.
    The framework tries to narrow down the skipped range by retrying
    until this threshold is met OR all attempts get exhausted for this task.
    Set the value to Long.MAX_VALUE to indicate that framework need not try to
    narrow down. Whatever groups(depends on application) get skipped are
    acceptable."
2,dfs.namenode.fs-limits.min-block-size,1048576,"Minimum block size in bytes, enforced by the Namenode at create
      time. This prevents the accidental creation of files with tiny block
      sizes (and thus many blocks), which can degrade performance. Support multiple
      size unit suffix(case insensitive), as described in dfs.blocksize."
3,fs.s3a.multipart.purge.age,86400,"Minimum age in seconds of multipart uploads to purge
    on startup if ""fs.s3a.multipart.purge"" is true"
4,dfs.balancer.getBlocks.min-block-size,10485760,"Minimum block threshold size in bytes to ignore when fetching a source's
    block list."
5,ipc.client.kill.max,10,Defines the maximum number of clients to disconnect in one go.
6,mapreduce.job.end-notification.retry.interval,1000,"The number of milliseconds the submitter of the job wants to
    wait before job end notification is retried if it fails. This is capped by
    mapreduce.job.end-notification.max.retry.interval"
7,dfs.datanode.scan.period.hours,504,"If this is positive, the DataNode will not scan any
        individual block more than once in the specified scan period.
        If this is negative, the block scanner is disabled.
        If this is set to zero, then the default value of 504 hours
        or 3 weeks is used. Prior versions of HDFS incorrectly documented
        that setting this key to zero will disable the block scanner."
8,mapreduce.job.cache.limit.max-single-resource-mb,0,"The maximum size (in MB) of a single resource a map reduce job
    is allow to submit for localization via files, libjars, archives, and
    jobjar command line arguments and through the distributed cache. If set to
    0 the limit is ignored."
9,dfs.namenode.decommission.backoff.monitor.pending.limit,10000,"When the Backoff monitor is enabled, determines the maximum number of blocks
    related to decommission and maintenance operations that can be loaded
    into the replication queue at any given time. Every
    dfs.namenode.decommission.interval seconds, the list is checked to see if
    the blocks have become fully replicated and then further blocks are added
    to reach the limit defined in this parameter."
