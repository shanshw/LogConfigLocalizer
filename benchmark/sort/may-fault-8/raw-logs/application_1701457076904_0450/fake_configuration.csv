,name,value,description
0,io.bytes.per.checksum,472103714,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,yarn.timeline-service.client.fd-clean-interval-secs,60,
2,dfs.namenode.redundancy.queue.restart.iterations,2400,"When picking blocks from the low redundancy queues, reset the
    bookmarked iterator after the set number of iterations to ensure any blocks
    which were not processed on the first pass are retried before the iterators
    would naturally reach their end point. This ensures blocks are retried
    more frequently when there are many pending blocks or blocks are
    continuously added to the queues preventing the iterator reaching its
    natural endpoint.
    The default setting of 2400 combined with the default of
    dfs.namenode.redundancy.interval.seconds means the iterators will be reset
    approximately every 2 hours.
    Setting this parameter to zero disables the feature and the iterators will
    be reset only when the end of all queues has been reached."
3,ipc.server.max.connections,0,"The maximum number of concurrent connections a server is allowed
    to accept. If this limit is exceeded, incoming connections will first fill
    the listen queue and then may go to an OS-specific listen overflow queue.
    The client may fail or timeout, but the server can avoid running out of file
    descriptors using this feature. 0 means no limit."
4,dfs.namenode.ec.policies.max.cellsize,4194304,The maximum cell size of erasure coding policy. Default is 4MB.
5,dfs.datanode.du.reserved,0,"Reserved space in bytes per volume. Always leave this much space free for non dfs use.
      Specific storage type based reservation is also supported. The property can be followed with
      corresponding storage types ([ssd]/[disk]/[archive]/[ram_disk]) for cluster with heterogeneous storage.
      For example, reserved space for RAM_DISK storage can be configured using property
      'dfs.datanode.du.reserved.ram_disk'. If specific storage type reservation is not configured
      then dfs.datanode.du.reserved will be used. Support multiple size unit suffix(case insensitive),
      as described in dfs.blocksize.
      Note: In case of using tune2fs to set reserved-blocks-percentage, or other filesystem tools,
      then you can possibly run into out of disk errors because hadoop will not check those
      external tool configurations."
6,ha.health-monitor.rpc.connect.max.retries,1,"The number of retries on connect error when establishing RPC proxy
    connection to NameNode, used for monitorHealth() calls."
7,dfs.namenode.blockreport.max.lock.hold.time,4,The BlockReportProcessingThread max write lock hold time in ms.
8,dfs.datanode.cache.revocation.timeout.ms,900000,"When the DFSClient reads from a block file which the DataNode is
    caching, the DFSClient can skip verifying checksums.  The DataNode will
    keep the block file in cache until the client is done.  If the client takes
    an unusually long time, though, the DataNode may need to evict the block
    file from the cache anyway.  This value controls how long the DataNode will
    wait for the client to release a replica that it is reading without
    checksums."
9,yarn.nodemanager.recovery.compaction-interval-secs,3600,
