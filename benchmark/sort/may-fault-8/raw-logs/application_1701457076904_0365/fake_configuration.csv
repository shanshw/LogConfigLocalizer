,name,value,description
0,io.bytes.per.checksum,0,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,fs.du.interval,600000,File space usage statistics refresh interval in msec.
2,ipc.[port_number].decay-scheduler.period-ms,5000,"How frequently the decay factor should be applied to the
    operation counts of users. Higher values have less overhead, but respond
    less quickly to changes in client behavior.
    This property applies to DecayRpcScheduler."
3,yarn.nm.liveness-monitor.expiry-interval-ms,600000,
4,fs.s3a.attempts.maximum,20,How many times we should retry commands on transient errors.
5,mapreduce.job.ubertask.maxreduces,1,"Threshold for number of reduces, beyond which job is considered
  too big for the ubertasking optimization.  CURRENTLY THE CODE CANNOT SUPPORT
  MORE THAN ONE REDUCE and will ignore larger values.  (Zero is a valid max,
  however.)  Users may override this value, but only downward."
6,hadoop.http.idle_timeout.ms,60000,NN/JN/DN Server connection timeout in milliseconds.
7,hadoop.fuse.timer.period,5,"The number of seconds between cache expiry checks in fuse_dfs. Lower values
    will result in fuse_dfs noticing changes to Kerberos ticket caches more
    quickly."
8,yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size,10000,
9,dfs.client.retry.window.base,3000,"Base time window in ms for DFSClient retries.  For each retry attempt,
    this value is extended linearly (e.g. 3000 ms for first attempt and
    first retry, 6000 ms for second retry, 9000 ms for third retry, etc.)."
