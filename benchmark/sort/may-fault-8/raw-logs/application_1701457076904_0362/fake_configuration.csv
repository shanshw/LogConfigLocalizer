,name,value,description
0,io.bytes.per.checksum,-1999303782,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,dfs.http.client.retry.max.attempts,10,"Specify the max number of retry attempts for WebHDFS client,
    if the difference between retried attempts and failovered attempts is
    larger than the max number of retry attempts, there will be no more
    retries."
2,yarn.dispatcher.cpu-monitor.samples-per-min,60,
3,dfs.datanode.ec.reconstruction.stripedread.timeout.millis,5000,Datanode striped read timeout in milliseconds.
4,dfs.namenode.max.extra.edits.segments.retained,10000,"The maximum number of extra edit log segments which should be retained
  beyond what is minimally necessary for a NN restart. When used in conjunction with
  dfs.namenode.num.extra.edits.retained, this configuration property serves to cap
  the number of extra edits files to a reasonable value."
5,dfs.ha.zkfc.port,8019,"The port number that the zookeeper failover controller RPC
    server binds to."
6,dfs.datanode.socket.write.timeout,480000,Timeout in ms for clients socket writes to DataNodes.
7,dfs.datanode.max.disks.to.report,5,"Number of disks to include in JSON report per operation. We will return
    disks with the highest latency."
8,ha.failover-controller.graceful-fence.rpc-timeout.ms,5000,Timeout that the FC waits for the old active to go to standby
9,mapreduce.job.speculative.minimum-allowed-tasks,10,"The minimum allowed tasks that
  can be speculatively re-executed at any time."
