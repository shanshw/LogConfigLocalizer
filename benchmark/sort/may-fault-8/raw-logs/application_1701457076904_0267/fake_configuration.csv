,name,value,description
0,io.bytes.per.checksum,0,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,hadoop.security.group.mapping.ldap.connection.timeout.ms,60000,"This property is the connection timeout (in milliseconds) for LDAP
    operations. If the LDAP provider doesn't establish a connection within the
    specified period, it will abort the connect attempt. Non-positive value
    means no LDAP connection timeout is specified in which case it waits for the
    connection to establish until the underlying network times out."
2,mapreduce.client.progressmonitor.pollinterval,1000,"The interval (in milliseconds) between which the JobClient
    reports status to the console and checks for job completion. You may want to set this
    to a lower value to make tests run faster on a single node system. Adjusting
    this value in production may lead to unwanted client-server traffic."
3,ipc.client.connect.timeout,20000,"Indicates the number of milliseconds a client will wait for the
               socket to establish a server connection."
4,mapreduce.input.fileinputformat.list-status.num-threads,1,"The number of threads to use to list and fetch block locations
  for the specified input paths. Note: multiple threads should not be used
  if a custom non thread-safe path filter is used."
5,hadoop.security.groups.cache.secs,300,"This is the config controlling the validity of the entries in the cache
    containing the user->group mapping. When this duration has expired,
    then the implementation of the group mapping provider is invoked to get
    the groups of the user and then cached back."
6,seq.io.sort.mb,100,"The total amount of buffer memory to use while sorting files,
      while using SequenceFile.Sorter, in megabytes. By default,
      gives each merge stream 1MB, which should minimize seeks."
7,hadoop.security.kms.client.encrypted.key.cache.expiry,43200000,"Cache expiry time for a Key, after which the cache Queue for this
    key will be dropped. Default = 12hrs"
8,fs.s3a.retry.limit,7,"Number of times to retry any repeatable S3 client request on failure,
    excluding throttling requests."
9,dfs.namenode.decommission.interval,30,"Namenode periodicity in seconds to check if
    decommission or maintenance is complete. Support multiple time unit
    suffix(case insensitive), as described in dfs.heartbeat.interval.
    If no time unit is specified then seconds is assumed."
