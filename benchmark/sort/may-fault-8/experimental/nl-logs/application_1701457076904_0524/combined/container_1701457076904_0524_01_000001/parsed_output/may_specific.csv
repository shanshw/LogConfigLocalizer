0,1
d7bfe329,job_<*>_0524Job Transitioned from <*> to <*>
9c9cab88,Instantiated MRClientService at ed761ad59d44<*>
cbd1ec4a,Logging initialized @3200ms to org.eclipse.jetty.util.log.Slf4jLog
64af699d,jetty-<*>.<*>.<*>.v20230217; built: <*>-<*>-17T08:<*>:<*>.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm <*><*>-post-Ubuntu-0ubuntu120.<*>
0fcdc0c2,node0 Scavenging every 600000ms
57d13d24,"Started ServerConnector@75fd65c{HTTP/<*>.<*>, (http/<*>.<*>)}{<*>}"
2585da6c,"Thread Thread[eventHandlingThread,<*>,main] threw an Exception.org.apache.hadoop.HadoopIllegalArgumentException: Invalid values: dfs.bytes-per-checksum (=<*>) must divide block size (=<*>). at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:<*>) at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:<*>) at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem$<*>.doCall(DistributedFileSystem.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem$<*>.doCall(DistributedFileSystem.java:<*>) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.createEventWriter(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.setupEventWriter(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$<*>.run(JobHistoryEventHandler.java:<*>) at java.base/java.lang.Thread.run(Thread.java:<*>)"
d1df58d8,Done acknowledgment from attempt_<*>_<*>_m_<*>_<*>
1aac72f8,Task succeeded with attempt attempt_<*>_<*>_m_<*>_<*>
d0481ad7,Task: attempt_<*>_<*>_r_<*>_<*> - exited : org.apache.hadoop.HadoopIllegalArgumentException: Invalid values: dfs.bytes-per-checksum (=<*>) must divide block size (=<*>). at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:<*>) at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:<*>) at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem$<*>.doCall(DistributedFileSystem.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem$<*>.doCall(DistributedFileSystem.java:<*>) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:<*>) at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:<*>) at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:<*>) at org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.getSequenceWriter(SequenceFileOutputFormat.java:<*>) at org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.getRecordWriter(SequenceFileOutputFormat.java:<*>) at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.<init>(ReduceTask.java:<*>) at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:<*>) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:<*>) at org.apache.hadoop.mapred.YarnChild$<*>.run(YarnChild.java:<*>) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:<*>) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<*>) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:<*>)
65bf2ff3,Diagnostics report from attempt_<*>_<*>_r_<*>_<*>: Error: org.apache.hadoop.HadoopIllegalArgumentException: Invalid values: dfs.bytes-per-checksum (=<*>) must divide block size (=<*>). at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:<*>) at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:<*>) at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem$<*>.doCall(DistributedFileSystem.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem$<*>.doCall(DistributedFileSystem.java:<*>) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:<*>) at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:<*>) at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:<*>) at org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.getSequenceWriter(SequenceFileOutputFormat.java:<*>) at org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat.getRecordWriter(SequenceFileOutputFormat.java:<*>) at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.<init>(ReduceTask.java:<*>) at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:<*>) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:<*>) at org.apache.hadoop.mapred.YarnChild$<*>.run(YarnChild.java:<*>) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:<*>) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<*>) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:<*>)
0e4be9d6,<*> failures on node <*>
b2abfdcd,"attempt_<*>_<*>_r_<*>_<*> transitioned from state FAIL_FINISHING_CONTAINER to FAILED, event type is <*> and <*>"
a0d2d61b,Blacklisted host <*>
20ca077e,Update the blacklist for application_<*>_<*>: blacklistAdditions=<*> blacklistRemovals=<*>
350cb79a,"Ignore blacklisting set to true. Known: <*>, Blacklisted: <*>, <*>%"
db25a59d,Job failed as tasks failed. failedMaps:<*> failedReduces:<*> killedMaps:<*> killedReduces: <*>
5971e380,"Cannot assign container Container: [ContainerId: container_<*>_<*>_<*>_<*>, AllocationRequestId: <*>, Version: <*>, NodeId: 2f08f873c798:<*>, NodeHttpAddress: 2f08f873c798:<*>, Resource: <memory:<*>, vCores:<*>>, Priority: <*>, Token: Token { kind: ContainerToken, service: <*> }, ExecutionType: GUARANTEED, ] for a reduce as either container memory less than required <memory:<*>, vCores:<*>> or no pending reduce tasks."
121a8bac,Could not delete hdfs://2f08f873c798:<*>/efghwordsortedGk1hIgPo/_ABS5voutput/_temporary/<*>/_temporary/attempt_<*>_<*>_r_<*>_<*>
6e5e38d1,"In stop, writing event JOB_SUBMITTED"
22664475,Service <*> failed in state STOPPEDjava.lang.NullPointerException at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:<*>) at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:<*>) at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:<*>) at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:<*>) at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:<*>) at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler$<*>.run(MRAppMaster.java:<*>)
79523fbd,When stopping the service JobHistoryEventHandlerjava.lang.NullPointerException at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:<*>) at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:<*>) at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:<*>) at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:<*>) at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:<*>) at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler$<*>.run(MRAppMaster.java:<*>)
ca66a1c8,Setting job diagnostics to Task failed task_<*>_<*>_r_000001Job failed as tasks failed. failedMaps:<*> failedReduces:<*> killedMaps:<*> killedReduces: <*>
03ca4927,History url is null
9ab47de7,Graceful stop failed. Exiting..java.lang.NullPointerException at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:<*>) at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:<*>) at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:<*>) at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:<*>) at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:<*>) at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler$<*>.run(MRAppMaster.java:<*>)
61bb61d6,Exiting with status <*>: java.lang.NullPointerException
