,name,value,description
0,io.bytes.per.checksum,0,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms,10000,
2,hadoop.shell.safely.delete.limit.num.files,100,"Used by -safely option of hadoop fs shell -rm command to avoid
      accidental deletion of large directories. When enabled, the -rm command
      requires confirmation if the number of files to be deleted is greater than
      this limit.  The default limit is 100 files. The warning is disabled if
      the limit is 0 or the -safely is not specified in -rm command."
3,hadoop.fuse.timer.period,5,"The number of seconds between cache expiry checks in fuse_dfs. Lower values
    will result in fuse_dfs noticing changes to Kerberos ticket caches more
    quickly."
4,dfs.image.transfer.bandwidthPerSec,52428800,"Maximum bandwidth used for regular image transfers (instead of
        bootstrapping the standby namenode), in bytes per second.
        This can help keep normal namenode operations responsive during
        checkpointing.
        A default value is 50mb per second.
        The maximum bandwidth used for bootstrapping standby namenode is
        configured with dfs.image.transfer-bootstrap-standby.bandwidthPerSec.
        Support multiple size unit suffix(case insensitive), as described
        in dfs.blocksize."
5,dfs.client.retry.window.base,3000,"Base time window in ms for DFSClient retries.  For each retry attempt,
    this value is extended linearly (e.g. 3000 ms for first attempt and
    first retry, 6000 ms for second retry, 9000 ms for third retry, etc.)."
6,yarn.nodemanager.delete.debug-delay-sec,0,
7,dfs.image.parallel.target.sections,12,"Controls the number of sub-sections that will be written to
        fsimage for each section. This should be larger than
        dfs.image.parallel.threads, otherwise all threads will not be
        used when loading. Ideally, have at least twice the number
        of target sections as threads, so each thread must load more
        than one section to avoid one long running section affecting
        the load time."
8,dfs.client.read.shortcircuit.streams.cache.expiry.ms,300000,"This controls the minimum amount of time
    file descriptors need to sit in the client cache context
    before they can be closed for being inactive for too long."
9,dfs.datanode.scan.period.hours,504,"If this is positive, the DataNode will not scan any
        individual block more than once in the specified scan period.
        If this is negative, the block scanner is disabled.
        If this is set to zero, then the default value of 504 hours
        or 3 weeks is used. Prior versions of HDFS incorrectly documented
        that setting this key to zero will disable the block scanner."
