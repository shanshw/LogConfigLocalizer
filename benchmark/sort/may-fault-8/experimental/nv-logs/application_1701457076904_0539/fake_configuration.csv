,name,value,description
0,io.bytes.per.checksum,0,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,dfs.client.write.exclude.nodes.cache.expiry.interval.millis,600000,"The maximum period to keep a DN in the excluded nodes list
  at a client. After this period, in milliseconds, the previously excluded node(s) will
  be removed automatically from the cache and will be considered good for block allocations
  again. Useful to lower or raise in situations where you keep a file open for very long
  periods (such as a Write-Ahead-Log (WAL) file) to make the writer tolerant to cluster maintenance
  restarts. Defaults to 10 minutes."
2,dfs.qjournal.http.open.timeout.ms,60000,"Timeout in milliseconds when open a new HTTP connection to remote
    journals."
3,dfs.namenode.edekcacheloader.initial.delay.ms,3000,"When KeyProvider is configured, the time delayed until the first
    attempt to warm up edek cache on NN start up / become active."
4,dfs.namenode.max-lock-hold-to-release-lease-ms,25,"During the release of lease a lock is hold that make any
    operations on the namenode stuck. In order to not block them during
    a too long duration we stop releasing lease after this max lock limit."
5,yarn.resourcemanager.scheduler.client.thread-count,50,
6,yarn.nodemanager.localizer.client.thread-count,5,
7,dfs.namenode.decommission.max.concurrent.tracked.nodes,100,"The maximum number of decommission-in-progress or
    entering-maintenance datanodes nodes that will be tracked at one time by
    the namenode. Tracking these datanode consumes additional NN memory
    proportional to the number of blocks on the datnode. Having a conservative
    limit reduces the potential impact of decommissioning or maintenance of
    a large number of nodes at once.
      
    A value of 0 means no limit will be enforced."
8,fs.s3a.list.version,2,"Select which version of the S3 SDK's List Objects API to use.  Currently
    support 2 (default) and 1 (older API)."
9,mapreduce.job.split.metainfo.maxsize,10000000,"The maximum permissible size of the split metainfo file.
  The MapReduce ApplicationMaster won't attempt to read submitted split metainfo
  files bigger than this configured value.
  No limits if set to -1."
