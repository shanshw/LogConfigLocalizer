,name,value,description
0,yarn.client.nodemanager-connect.retry-interval-ms,0.60129297,<missing>
1,mapreduce.reduce.shuffle.fetch.retry.interval-ms,1000,"Time of interval that fetcher retry to fetch again when some
  non-fatal failure happens because of some events like NM restart."
2,fs.s3a.executor.capacity,16,"The maximum number of submitted tasks which is a single
    operation (e.g. rename(), delete()) may submit simultaneously for
    execution -excluding the IO-heavy block uploads, whose capacity
    is set in ""fs.s3a.fast.upload.active.blocks""

    All tasks are submitted to the shared thread pool whose size is
    set in ""fs.s3a.threads.max""; the value of capacity should be less than that
    of the thread pool itself, as the goal is to stop a single operation
    from overloading that thread pool."
3,dfs.client.max.block.acquire.failures,3,Maximum failures allowed when trying to get block information from a specific datanode.
4,dfs.datanode.max.nodes.to.report,5,"Number of nodes to include in JSON report. We will return nodes with
    the highest number of votes from peers."
5,fs.s3a.retry.limit,7,"Number of times to retry any repeatable S3 client request on failure,
    excluding throttling requests."
6,dfs.webhdfs.ugi.expire.after.access,600000,"How long in milliseconds after the last access
      the cached UGI will expire. With 0, never expire."
7,yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms,1000,"The interval in ms at which the MR AppMaster should send
    heartbeats to the ResourceManager"
8,hadoop.security.group.mapping.ldap.read.timeout.ms,60000,"This property is the read timeout (in milliseconds) for LDAP
    operations. If the LDAP provider doesn't get a LDAP response within the
    specified period, it will abort the read attempt. Non-positive value
    means no read timeout is specified in which case it waits for the response
    infinitely."
9,dfs.datanode.fsdatasetasyncdisk.max.threads.per.volume,4,"The maximum number of threads per volume used to process async disk
    operations on the datanode. These threads consume I/O and CPU at the
    same time. This will affect normal data node operations."
