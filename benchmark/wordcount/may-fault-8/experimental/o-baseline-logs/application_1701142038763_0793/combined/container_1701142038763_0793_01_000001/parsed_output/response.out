name:dfs.client.mmap.retry.timeout.ms
value:0
relevant log:1-WARN Existing client context 'default' does not match requested configuration. Existing: shortCircuitStreamsCacheSize = 256, shortCircuitStreamsCacheExpiryMs = 300000, shortCircuitMmapCacheSize = 256, shortCircuitMmapCacheExpiryMs = 3600000, shortCircuitMmapCacheRetryTimeout = 0, shortCircuitCacheStaleThresholdMs = 1800000, socketCacheCapacity = 16, socketCacheExpiry = 3000, shortCircuitLocalReads = false, useLegacyBlockReaderLocal = false, domainSocketDataTraffic = false, shortCircuitSharedMemoryWatcherInterruptCheckMs = 60000, keyProviderCacheExpiryMs = 864000000, domainSocketDisableIntervalSeconds = 600, Requested: shortCircuitStreamsCacheSize = 256, shortCircuitStreamsCacheExpiryMs = 300000, shortCircuitMmapCacheSize = 256, shortCircuitMmapCacheExpiryMs = 3600000, shortCircuitMmapCacheRetryTimeout = 300000, shortCircuitCacheStaleThresholdMs = 1800000, socketCacheCapacity = 16, socketCacheExpiry = 3000, shortCircuitLocalReads = false, useLegacyBlockReaderLocal = false, domainSocketDataTraffic = false, shortCircuitSharedMemoryWatcherInterruptCheckMs = 60000, keyProviderCacheExpiryMs = 864000000, domainSocketDisableIntervalSeconds = 600
explanation: The log indicates that the existing configuration does not match the requested configuration. The dfs.client.mmap.retry.timeout.ms is set to 0 in the existing configuration, but the requested configuration requires it to be 300000. This mismatch could be the cause of the warning.

name:yarn.app.mapreduce.am.container.log.limit.kb
value:0
relevant log:<missing>
explanation: The value of yarn.app.mapreduce.am.container.log.limit.kb is set to 0, which disables the cap on the maximum size of the MRAppMaster attempt container logs. This could potentially lead to excessive logging and consume a lot of disk space.

name:dfs.namenode.lazypersist.file.scrub.interval.sec
value:300
relevant log:<missing>
explanation: The dfs.namenode.lazypersist.file.scrub.interval.sec is set to 300. This configuration controls the interval between successive scans for LazyPersist files with missing blocks. If there are many missing blocks, this could potentially lead to a delay in detecting and resolving the issue.