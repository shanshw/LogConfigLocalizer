,name,value,description
0,io.bytes.per.checksum,0,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,dfs.qjournal.parallel-read.num-threads,5,Number of threads per JN to be used for tailing edits.
2,dfs.client.write.byte-array-manager.count-reset-time-period-ms,10000,"The time period in milliseconds that the allocation count for each array length is
    reset to zero if there is no increment."
3,dfs.namenode.fs-limits.max-xattr-size,16384,"The maximum combined size of the name and value of an extended attribute
    in bytes. It should be larger than 0, and less than or equal to maximum
    size hard limit which is 32768.
    Support multiple size unit suffix(case insensitive), as described in
    dfs.blocksize."
4,dfs.client.server-defaults.validity.period.ms,3600000,"The amount of milliseconds after which cached server defaults are updated.

    By default this parameter is set to 1 hour.
    Support multiple time unit suffix(case insensitive), as described
    in dfs.heartbeat.interval."
5,mapreduce.map.skip.maxrecords,0,"The number of acceptable skip records surrounding the bad
    record PER bad record in mapper. The number includes the bad record as well.
    To turn the feature of detection/skipping of bad records off, set the
    value to 0.
    The framework tries to narrow down the skipped range by retrying
    until this threshold is met OR all attempts get exhausted for this task.
    Set the value to Long.MAX_VALUE to indicate that framework need not try to
    narrow down. Whatever records(depends on application) get skipped are
    acceptable."
6,yarn.scheduler.configuration.store.max-logs,1000,
7,yarn.resourcemanager.reservation-system.planfollower.time-step,1000,
8,mapreduce.jobhistory.intermediate-user-done-dir.permissions,770,"The permissions of the user directories in
  ${mapreduce.jobhistory.intermediate-done-dir}. The user and the group
  permission must be 7, this is enforced."
9,dfs.datanode.max.nodes.to.report,5,"Number of nodes to include in JSON report. We will return nodes with
    the highest number of votes from peers."
