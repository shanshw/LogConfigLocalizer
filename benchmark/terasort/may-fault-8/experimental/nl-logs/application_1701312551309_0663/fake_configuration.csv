,name,value,description
0,io.bytes.per.checksum,0,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,dfs.ha.tail-edits.period,60,"How often, the StandbyNode and ObserverNode should check if there are new
    edit log entries ready to be consumed. This is the minimum period between
    checking; exponential backoff will be applied if no edits are found and
    dfs.ha.tail-edits.period.backoff-max is configured. By default, no
    backoff is applied.
    Supports multiple time unit suffix (case insensitive), as described
    in dfs.heartbeat.interval."
2,dfs.namenode.snapshot.skiplist.max.levels,0,"Maximum no of the skip levels to be maintained in the skip list for
    storing directory snapshot diffs. By default, it is set to 0 and a linear
    list will be used to store the directory snapshot diffs."
3,yarn.scheduler.maximum-allocation-vcores,4,
4,fs.s3a.connection.maximum,96,"Controls the maximum number of simultaneous connections to S3.
    This must be bigger than the value of fs.s3a.threads.max so as to stop
    threads being blocked waiting for new HTTPS connections.
    Why not equal? The AWS SDK transfer manager also uses these connections."
5,io.seqfile.compress.blocksize,1000000,"The minimum block size for compression in block compressed
          SequenceFiles."
6,hadoop.security.dns.log-slow-lookups.threshold.ms,1000,"If slow lookup logging is enabled, this threshold is used to decide if a
    lookup is considered slow enough to be logged."
7,yarn.nodemanager.node-attributes.provider.fetch-interval-ms,600000,
8,dfs.storage.policy.satisfier.retry.max.attempts,3,"Max retry to satisfy the block storage policy. After this retry block will be removed
    from the movement needed queue."
9,dfs.client.write.byte-array-manager.count-limit,2048,The maximum number of arrays allowed for each array length.
