,name,value,description
0,io.bytes.per.checksum,0,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,ha.health-monitor.check-interval.ms,1000,How often to check the service.
2,dfs.heartbeat.interval,3,"Determines datanode heartbeat interval in seconds.
    Can use the following suffix (case insensitive):
    ms(millis), s(sec), m(min), h(hour), d(day)
    to specify the time (such as 2s, 2m, 1h, etc.).
    Or provide complete number in seconds (such as 30 for 30 seconds).
    If no time unit is specified then seconds is assumed."
3,mapreduce.reduce.cpu.vcores,1,"The number of virtual cores to request from the scheduler for
  each reduce task."
4,dfs.http.client.failover.max.attempts,15,"Specify the max number of failover attempts for WebHDFS client
    in case of network exception."
5,dfs.namenode.snapshot.skiplist.max.levels,0,"Maximum no of the skip levels to be maintained in the skip list for
    storing directory snapshot diffs. By default, it is set to 0 and a linear
    list will be used to store the directory snapshot diffs."
6,mapreduce.job.reducer.preempt.delay.sec,0,"The threshold (in seconds) after which an unsatisfied
      mapper request triggers reducer preemption when there is no anticipated
      headroom. If set to 0 or a negative value, the reducer is preempted as
      soon as lack of headroom is detected. Default is 0."
7,dfs.mover.max-no-move-interval,60000,"If this specified amount of time has elapsed and no block has been moved
    out of a source DataNode, on more effort will be made to move blocks out of
    this DataNode in the current Mover iteration."
8,dfs.namenode.get-blocks.max-qps,20,"The maximum number of getBlocks RPCs data movement utilities can make to
    a NameNode per second. Values less than or equal to 0 disable throttling.
    This affects anything that uses a NameNodeConnector, i.e., the Balancer,
    Mover, and StoragePolicySatisfier."
9,mapreduce.map.maxattempts,4,"Expert: The maximum number of attempts per map task.
  In other words, framework will try to execute a map task these many number
  of times before giving up on it."
