,name,value,description
0,io.bytes.per.checksum,333698898,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,dfs.client.failover.max.attempts,15,"Expert only. The number of client failover attempts that should be
    made before the failover is considered failed."
2,dfs.datanode.cached-dfsused.check.interval.ms,600000,"The interval check time of loading DU_CACHE_FILE in each volume.
    When the cluster doing the rolling upgrade operations, it will
    usually lead dfsUsed cache file of each volume expired and redo the
    du operations in datanode and that makes datanode start slowly. Adjust
    this property can make cache file be available for the time as you want."
3,dfs.http.client.failover.sleep.max.millis,15000,"Specify the upper bound of sleep time in milliseconds between
    retries or failovers for WebHDFS client."
4,dfs.namenode.block.deletion.increment,1000,"The number of block deletion increment.
      This setting will control the block increment deletion rate to
      ensure that other waiters on the lock can get in."
5,yarn.nodemanager.container-diagnostics-maximum-size,10000,
6,ftp.client-write-packet-size,65536,Packet size for clients to write
7,dfs.namenode.max.full.block.report.leases,6,"The maximum number of leases for full block reports that the
    NameNode will issue at any given time.  This prevents the NameNode from
    being flooded with full block reports that use up all the RPC handler
    threads.  This number should never be more than the number of RPC handler
    threads or less than 1."
8,dfs.replication,3,"Default block replication. 
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time."
9,yarn.app.mapreduce.client.max-retries,3,"The number of client retries to the RM/HS before
    throwing exception. This is a layer above the ipc."
