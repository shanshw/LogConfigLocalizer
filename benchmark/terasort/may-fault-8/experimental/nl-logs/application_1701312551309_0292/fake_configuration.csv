,name,value,description
0,dfs.client.key.provider.cache.expiry,0,DFS client security key cache expiration in milliseconds.
1,hadoop.registry.zk.retry.ceiling.ms,60000,"Zookeeper retry limit in milliseconds, during
      exponential backoff.

      This places a limit even
      if the retry times and interval limit, combined
      with the backoff policy, result in a long retry
      period"
2,seq.io.sort.mb,100,"The total amount of buffer memory to use while sorting files,
      while using SequenceFile.Sorter, in megabytes. By default,
      gives each merge stream 1MB, which should minimize seeks."
3,ipc.[port_number].weighted-cost.handler,1,"The weight multiplier to apply to the time spent in the
    HANDLER phase which do not involve holding a lock.
    See org.apache.hadoop.ipc.ProcessingDetails.Timing for more details on
    this phase. This property applies to WeightedTimeCostProvider."
4,dfs.storage.policy.satisfier.max.outstanding.paths,10000,"Defines the maximum number of paths to satisfy that can be queued up in the
    Satisfier call queue in a period of time. Default value is 10000."
5,yarn.resourcemanager.application.max-tags,10,
6,yarn.resourcemanager.nm-container-queuing.max-queue-wait-time-ms,100,
7,yarn.app.mapreduce.am.job.committer.cancel-timeout,60000,"The amount of time in milliseconds to wait for the output
    committer to cancel an operation if the job is killed"
8,dfs.disk.balancer.block.tolerance.percent,10,"When a disk balancer copy operation is proceeding, the datanode is still
      active. So it might not be possible to move the exactly specified
      amount of data. So tolerance allows us to define a percentage which
      defines a good enough move."
9,mapreduce.reduce.maxattempts,4,"Expert: The maximum number of attempts per reduce task.
  In other words, framework will try to execute a reduce task these many number
  of times before giving up on it."
