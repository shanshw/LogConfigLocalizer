,name,value,description
0,mapreduce.job.ubertask.maxmaps,132806309,"Threshold for number of maps, beyond which job is considered
  too big for the ubertasking optimization.  Users may override this value,
  but only downward."
1,yarn.resourcemanager.fs.state-store.num-retries,0,
2,fs.s3a.executor.capacity,16,"The maximum number of submitted tasks which is a single
    operation (e.g. rename(), delete()) may submit simultaneously for
    execution -excluding the IO-heavy block uploads, whose capacity
    is set in ""fs.s3a.fast.upload.active.blocks""

    All tasks are submitted to the shared thread pool whose size is
    set in ""fs.s3a.threads.max""; the value of capacity should be less than that
    of the thread pool itself, as the goal is to stop a single operation
    from overloading that thread pool."
3,yarn.resourcemanager.placement-constraints.retry-attempts,3,
4,mapreduce.task.exit.timeout,60000,"The number of milliseconds before a task will be
  terminated if it stays in finishing state for too long.
  After a task attempt completes from TaskUmbilicalProtocol's point of view,
  it will be transitioned to finishing state. That will give a chance for the
  task to exit by itself."
5,dfs.datanode.fsdatasetcache.max.threads.per.volume,4,"The maximum number of threads per volume to use for caching new data
    on the datanode. These threads consume both I/O and CPU. This can affect
    normal datanode operations."
6,dfs.namenode.read-lock-reporting-threshold-ms,5000,"When a read lock is held on the namenode for a long time,
    this will be logged as the lock is released. This sets how long the
    lock must be held for logging to occur."
7,yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size,10,
8,yarn.nodemanager.node-labels.provider.fetch-interval-ms,600000,
9,yarn.nodemanager.node-attributes.provider.fetch-interval-ms,600000,
