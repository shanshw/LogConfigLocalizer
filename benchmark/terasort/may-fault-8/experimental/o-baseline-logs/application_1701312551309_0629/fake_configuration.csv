,name,value,description
0,io.bytes.per.checksum,0,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,yarn.nodemanager.health-checker.interval-ms,600000,
2,dfs.namenode.maintenance.replication.min,1,Minimal live block replication in existence of maintenance mode.
3,yarn.app.mapreduce.client.job.max-retries,3,"The number of retries the client will make for getJob and
    dependent calls.
    This is needed for non-HDFS DFS where additional, high level
    retries are required to avoid spurious failures during the getJob call.
    30 is a good value for WASB"
4,file.blocksize,67108864,Block size
5,mapreduce.job.local-fs.single-disk-limit.check.interval-ms,5000,Interval of disk limit check to run in ms.
6,ftp.bytes-per-checksum,512,"The number of bytes per checksum.  Must not be larger than
  ftp.stream-buffer-size"
7,dfs.datanode.fsdatasetcache.max.threads.per.volume,4,"The maximum number of threads per volume to use for caching new data
    on the datanode. These threads consume both I/O and CPU. This can affect
    normal datanode operations."
8,dfs.datanode.data.dir.perm,700,"Permissions for the directories on on the local filesystem where
  the DFS data node store its blocks. The permissions can either be octal or
  symbolic."
9,dfs.namenode.max.op.size,52428800,Maximum opcode size in bytes.
