,name,value,description
0,io.bytes.per.checksum,-404609460,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,yarn.resourcemanager.delegation-token.max-conf-size-bytes,12800,
2,dfs.datanode.data.dir.perm,700,"Permissions for the directories on on the local filesystem where
  the DFS data node store its blocks. The permissions can either be octal or
  symbolic."
3,yarn.nodemanager.resource.percentage-physical-cpu-limit,100,
4,mapreduce.map.skip.maxrecords,0,"The number of acceptable skip records surrounding the bad
    record PER bad record in mapper. The number includes the bad record as well.
    To turn the feature of detection/skipping of bad records off, set the
    value to 0.
    The framework tries to narrow down the skipped range by retrying
    until this threshold is met OR all attempts get exhausted for this task.
    Set the value to Long.MAX_VALUE to indicate that framework need not try to
    narrow down. Whatever records(depends on application) get skipped are
    acceptable."
5,mapreduce.shuffle.max.threads,0,"Max allowed threads for serving shuffle connections. Set to zero
  to indicate the default of 2 times the number of available
  processors (as reported by Runtime.availableProcessors()). Netty is used to
  serve requests, so a thread is not needed for each connection."
6,dfs.namenode.fs-limits.max-blocks-per-file,10000,"Maximum number of blocks per file, enforced by the Namenode on
        write. This prevents the creation of extremely large files which can
        degrade performance."
7,dfs.block.invalidate.limit,1000,"The maximum number of invalidate blocks sent by namenode to a datanode
    per heartbeat deletion command. This property works with
    ""dfs.namenode.invalidate.work.pct.per.iteration"" to throttle block
    deletions."
8,fs.ftp.timeout,0,FTP filesystem's timeout in seconds.
9,dfs.http.client.retry.max.attempts,10,"Specify the max number of retry attempts for WebHDFS client,
    if the difference between retried attempts and failovered attempts is
    larger than the max number of retry attempts, there will be no more
    retries."
