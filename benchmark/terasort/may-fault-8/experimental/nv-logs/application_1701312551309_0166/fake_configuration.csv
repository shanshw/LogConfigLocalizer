,name,value,description
0,io.bytes.per.checksum,66335827,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,dfs.namenode.decommission.blocks.per.interval,500000,"The approximate number of blocks to process per decommission
    or maintenance interval, as defined in dfs.namenode.decommission.interval."
2,tfile.fs.input.buffer.size,262144,Buffer size used for FSDataInputStream in bytes.
3,dfs.namenode.available-space-block-placement-policy.balanced-space-tolerance,5,"Only used when the dfs.block.replicator.classname is set to
      org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.
      Special value between 0 and 20, inclusive. if the value is set beyond the scope,
      this value will be set as 5 by default, Increases tolerance of
      placing blocks on Datanodes with similar disk space used."
4,mapreduce.job.counters.max,120,The max number of user counters allowed per job.
5,ha.health-monitor.connect-retry-interval.ms,1000,How often to retry connecting to the service.
6,dfs.client.slow.io.warning.threshold.ms,30000,"The threshold in milliseconds at which we will log a slow
    io warning in a dfsclient. By default, this parameter is set to 30000
    milliseconds (30 seconds)."
7,dfs.balancer.getBlocks.size,2147483648,"Total size in bytes of Datanode blocks to get when fetching a source's
    block list."
8,mapreduce.reduce.shuffle.retry-delay.max.ms,60000,"The maximum number of ms the reducer will delay before retrying
  to download map data."
9,dfs.client.deadnode.detection.probe.deadnode.threads,10,The maximum number of threads to use for probing dead node.
