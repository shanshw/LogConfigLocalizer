,name,value,description
0,io.bytes.per.checksum,-2145503254,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,mapreduce.reduce.shuffle.connect.timeout,180000,"Expert: The maximum amount of time (in milli seconds) reduce
  task spends in trying to connect to a remote node for getting map output."
2,yarn.nm.liveness-monitor.expiry-interval-ms,600000,
3,net.topology.script.number.args,100,"The max number of args that the script configured with
    net.topology.script.file.name should be run with. Each arg is an
    IP address."
4,dfs.namenode.max.op.size,52428800,Maximum opcode size in bytes.
5,dfs.mover.retry.max.attempts,10,"The maximum number of retries before the mover consider the
    move failed."
6,yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb,0,
7,mapreduce.job.cache.limit.max-resources-mb,0,"The maximum size (in MB) a map reduce job is allowed to submit
    for localization via files, libjars, archives, and jobjar command line
    arguments and through the distributed cache. If set to 0 the limit is
    ignored."
8,hadoop.security.kms.client.failover.sleep.base.millis,100,"Expert only. The time to wait, in milliseconds, between failover
    attempts increases exponentially as a function of the number of
    attempts made so far, with a random factor of +/- 50%. This option
    specifies the base value used in the failover calculation. The
    first failover will retry immediately. The 2nd failover attempt
    will delay at least hadoop.security.client.failover.sleep.base.millis
    milliseconds. And so on."
