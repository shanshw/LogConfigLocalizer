,name,value,description
0,io.bytes.per.checksum,-308252846,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,io.seqfile.compress.blocksize,1000000,"The minimum block size for compression in block compressed
          SequenceFiles."
2,dfs.datanode.failed.volumes.tolerated,0,"The number of volumes that are allowed to
  fail before a datanode stops offering service. By default
  any volume failure will cause a datanode to shutdown.
  The value should be greater than or equal to -1 , -1 represents minimum
  1 valid volume."
3,mapreduce.shuffle.pathcache.concurrency-level,16,"Uses the concurrency level to create a fixed number of hashtable
    segments, each governed by its own write lock."
4,dfs.client.write.byte-array-manager.count-threshold,128,"The count threshold for each array length so that a manager is created only after the
    allocation count exceeds the threshold. In other words, the particular array length
    is not managed until the allocation count exceeds the threshold."
5,hadoop.security.uid.cache.secs,14400,"This is the config controlling the validity of the entries in the cache
        containing the userId to userName and groupId to groupName used by
        NativeIO getFstat()."
6,dfs.namenode.decommission.blocks.per.interval,500000,"The approximate number of blocks to process per decommission
    or maintenance interval, as defined in dfs.namenode.decommission.interval."
7,yarn.resourcemanager.application.max-tags,10,
8,mapreduce.reduce.skip.maxgroups,0,"The number of acceptable skip groups surrounding the bad
    group PER bad group in reducer. The number includes the bad group as well.
    To turn the feature of detection/skipping of bad groups off, set the
    value to 0.
    The framework tries to narrow down the skipped range by retrying
    until this threshold is met OR all attempts get exhausted for this task.
    Set the value to Long.MAX_VALUE to indicate that framework need not try to
    narrow down. Whatever groups(depends on application) get skipped are
    acceptable."
9,dfs.datanode.ec.reconstruction.stripedread.buffer.size,65536,Datanode striped read buffer size.
