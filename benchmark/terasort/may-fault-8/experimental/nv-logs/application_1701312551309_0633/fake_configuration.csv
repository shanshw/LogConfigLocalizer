,name,value,description
0,io.bytes.per.checksum,1839460189,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,ipc.[port_number].decay-scheduler.period-ms,5000,"How frequently the decay factor should be applied to the
    operation counts of users. Higher values have less overhead, but respond
    less quickly to changes in client behavior.
    This property applies to DecayRpcScheduler."
2,dfs.datanode.data.transfer.bandwidthPerSec,0,"Specifies the maximum amount of bandwidth that the data transfering can utilize for transfering block when
      BlockConstructionStage is
      PIPELINE_SETUP_CREATE and clientName is empty.
      When the bandwidth value is zero, there is no limit."
3,mapreduce.map.skip.maxrecords,0,"The number of acceptable skip records surrounding the bad
    record PER bad record in mapper. The number includes the bad record as well.
    To turn the feature of detection/skipping of bad records off, set the
    value to 0.
    The framework tries to narrow down the skipped range by retrying
    until this threshold is met OR all attempts get exhausted for this task.
    Set the value to Long.MAX_VALUE to indicate that framework need not try to
    narrow down. Whatever records(depends on application) get skipped are
    acceptable."
4,yarn.timeline-service.entity-group-fs-store.scan-interval-seconds,60,
5,yarn.resourcemanager.scheduler.client.thread-count,50,
6,dfs.namenode.name.cache.threshold,10,"Frequently accessed files that are accessed more times than this
    threshold are cached in the FSDirectory nameCache."
7,yarn.nodemanager.aux-services.manifest.reload-ms,0,
8,hadoop.registry.zk.session.timeout.ms,60000,Zookeeper session timeout in milliseconds
9,dfs.image.transfer.chunksize,65536,"Chunksize in bytes to upload the checkpoint.
        Chunked streaming is used to avoid internal buffering of contents
        of image file of huge size.
        Support multiple size unit suffix(case insensitive), as described
        in dfs.blocksize."
