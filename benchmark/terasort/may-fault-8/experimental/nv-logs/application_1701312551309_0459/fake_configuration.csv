,name,value,description
0,io.bytes.per.checksum,0,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,ha.failover-controller.graceful-fence.rpc-timeout.ms,5000,Timeout that the FC waits for the old active to go to standby
2,yarn.router.pipeline.cache-max-size,25,
3,dfs.storage.policy.satisfier.recheck.timeout.millis,60000,"Blocks storage movements monitor re-check interval in milliseconds.
    This check will verify whether any blocks storage movement results arrived from DN
    and also verify if any of file blocks movements not at all reported to DN
    since dfs.storage.policy.satisfier.self.retry.timeout.
    The default value is 1 * 60 * 1000 (1 mins)"
4,yarn.nodemanager.opportunistic-containers-max-queue-length,0,
5,ipc.[port_number].weighted-cost.lockshared,10,"The weight multiplier to apply to the time spent in the
    processing phase which holds a shared (read) lock.
    This property applies to WeightedTimeCostProvider."
6,dfs.client.mmap.cache.size,256,"When zero-copy reads are used, the DFSClient keeps a cache of recently used
    memory mapped regions.  This parameter controls the maximum number of
    entries that we will keep in that cache.

    The larger this number is, the more file descriptors we will potentially
    use for memory-mapped files.  mmaped files also use virtual address space.
    You may need to increase your ulimit virtual address space limits before
    increasing the client mmap cache size.

    Note that you can still do zero-copy reads when this size is set to 0."
7,dfs.namenode.redundancy.interval.seconds,3,"The periodicity in seconds with which the namenode computes 
  low redundancy work for datanodes. Support multiple time unit suffix(case insensitive),
  as described in dfs.heartbeat.interval."
8,dfs.namenode.safemode.extension,30000,"Determines extension of safe mode in milliseconds after the threshold level
    is reached.  Support multiple time unit suffix (case insensitive), as
    described in dfs.heartbeat.interval."
9,dfs.client.block.write.locateFollowingBlock.retries,5,Number of retries to use when finding the next block during HDFS writes.
