,name,value,description
0,mapreduce.task.io.sort.mb,0,"The total amount of buffer memory to use while sorting
  files, in megabytes.  By default, gives each merge stream 1MB, which
  should minimize seeks."
1,hadoop.fuse.timer.period,5,"The number of seconds between cache expiry checks in fuse_dfs. Lower values
    will result in fuse_dfs noticing changes to Kerberos ticket caches more
    quickly."
2,dfs.namenode.decommission.interval,30,"Namenode periodicity in seconds to check if
    decommission or maintenance is complete. Support multiple time unit
    suffix(case insensitive), as described in dfs.heartbeat.interval.
    If no time unit is specified then seconds is assumed."
3,yarn.sharedcache.uploader.server.thread-count,50,
4,dfs.mover.moverThreads,1000,Configure the balancer's mover thread pool size.
5,yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms,1000,"The interval in ms at which the MR AppMaster should send
    heartbeats to the ResourceManager"
6,yarn.timeline-service.leveldb-timeline-store.read-cache-size,104857600,
7,dfs.namenode.decommission.max.concurrent.tracked.nodes,100,"The maximum number of decommission-in-progress or
    entering-maintenance datanodes nodes that will be tracked at one time by
    the namenode. Tracking these datanode consumes additional NN memory
    proportional to the number of blocks on the datnode. Having a conservative
    limit reduces the potential impact of decommissioning or maintenance of
    a large number of nodes at once.
      
    A value of 0 means no limit will be enforced."
8,dfs.balancer.service.retries.on.exception,5,"When the balancer is executed as a long-running service, it will retry upon encountering an exception. This
    configuration determines how many times it will retry before considering the exception to be fatal and quitting."
9,mapreduce.fileoutputcommitter.algorithm.version,2,"The file output committer algorithm version
  valid algorithm version number: 1 or 2
  default to 2, which is the original algorithm

  In algorithm version 1,

  1. commitTask will rename directory
  $joboutput/_temporary/$appAttemptID/_temporary/$taskAttemptID/
  to
  $joboutput/_temporary/$appAttemptID/$taskID/

  2. recoverTask will also do a rename
  $joboutput/_temporary/$appAttemptID/$taskID/
  to
  $joboutput/_temporary/($appAttemptID + 1)/$taskID/

  3. commitJob will merge every task output file in
  $joboutput/_temporary/$appAttemptID/$taskID/
  to
  $joboutput/, then it will delete $joboutput/_temporary/
  and write $joboutput/_SUCCESS

  It has a performance regression, which is discussed in MAPREDUCE-4815.
  If a job generates many files to commit then the commitJob
  method call at the end of the job can take minutes.
  the commit is single-threaded and waits until all
  tasks have completed before commencing.

  algorithm version 2 will change the behavior of commitTask,
  recoverTask, and commitJob.

  1. commitTask will rename all files in
  $joboutput/_temporary/$appAttemptID/_temporary/$taskAttemptID/
  to $joboutput/

  2. recoverTask actually doesn't require to do anything, but for
  upgrade from version 1 to version 2 case, it will check if there
  are any files in
  $joboutput/_temporary/($appAttemptID - 1)/$taskID/
  and rename them to $joboutput/

  3. commitJob can simply delete $joboutput/_temporary and write
  $joboutput/_SUCCESS

  This algorithm will reduce the output commit time for
  large jobs by having the tasks commit directly to the final
  output directory as they were completing and commitJob had
  very little to do."
