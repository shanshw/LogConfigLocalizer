,name,value,description
0,io.bytes.per.checksum,-1012936192,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,yarn.app.mapreduce.am.container.log.backups,0,"Number of backup files for the ApplicationMaster logs when using
    ContainerRollingLogAppender (CRLA). See
    org.apache.log4j.RollingFileAppender.maxBackupIndex. By default,
    ContainerLogAppender (CLA) is used, and container logs are not rolled. CRLA
    is enabled for the ApplicationMaster when both
    yarn.app.mapreduce.am.container.log.limit.kb and
    yarn.app.mapreduce.am.container.log.backups are greater than zero."
2,nfs.wtmax,1048576,"This is the maximum size in bytes of a WRITE request
    supported by the NFS gateway. If you change this, make sure you
    also update the nfs mount's wsize(add wsize= # of bytes to the 
    mount directive)."
3,dfs.namenode.top.window.num.buckets,10,Number of buckets in the rolling window implementation of nntop
4,yarn.nodemanager.container-log-monitor.dir-size-limit-bytes,1000000000,
5,dfs.datanode.restart.replica.expiration,50,"During shutdown for restart, the amount of time in seconds budgeted for
    datanode restart."
6,hadoop.security.crypto.buffer.size,8192,The buffer size used by CryptoInputStream and CryptoOutputStream.
7,dfs.client.max.block.acquire.failures,3,Maximum failures allowed when trying to get block information from a specific datanode.
8,hadoop.registry.zk.retry.ceiling.ms,60000,"Zookeeper retry limit in milliseconds, during
      exponential backoff.

      This places a limit even
      if the retry times and interval limit, combined
      with the backoff policy, result in a long retry
      period"
9,mapreduce.map.cpu.vcores,1,"The number of virtual cores to request from the scheduler for
  each map task."
