,name,value,description
0,io.bytes.per.checksum,1293771670,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,ha.failover-controller.graceful-fence.rpc-timeout.ms,5000,Timeout that the FC waits for the old active to go to standby
2,mapreduce.task.userlog.limit.kb,0,The maximum size of user-logs of each task in KB. 0 disables the cap.
3,dfs.ha.fencing.ssh.connect-timeout,30000,"SSH connection timeout, in milliseconds, to use with the builtin
    sshfence fencer."
4,yarn.nodemanager.localizer.cache.target-size-mb,10240,
5,fs.s3a.fast.upload.active.blocks,4,"Maximum Number of blocks a single output stream can have
    active (uploading, or queued to the central FileSystem
    instance's pool of queued operations.

    This stops a single stream overloading the shared thread pool."
6,mapreduce.job.split.metainfo.maxsize,10000000,"The maximum permissible size of the split metainfo file.
  The MapReduce ApplicationMaster won't attempt to read submitted split metainfo
  files bigger than this configured value.
  No limits if set to -1."
7,yarn.nodemanager.localizer.cache.cleanup.interval-ms,600000,
8,dfs.datanode.scan.period.hours,504,"If this is positive, the DataNode will not scan any
        individual block more than once in the specified scan period.
        If this is negative, the block scanner is disabled.
        If this is set to zero, then the default value of 504 hours
        or 3 weeks is used. Prior versions of HDFS incorrectly documented
        that setting this key to zero will disable the block scanner."
9,ipc.[port_number].decay-scheduler.period-ms,5000,"How frequently the decay factor should be applied to the
    operation counts of users. Higher values have less overhead, but respond
    less quickly to changes in client behavior.
    This property applies to DecayRpcScheduler."
