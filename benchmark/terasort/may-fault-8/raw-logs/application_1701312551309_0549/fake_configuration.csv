,name,value,description
0,yarn.app.mapreduce.shuffle.log.limit.kb,PwGKZ,"Maximum size of the syslog.shuffle file in kilobytes
    (0 for no limit)."
1,yarn.resourcemanager.max-completed-applications,1000,
2,ftp.replication,3,Replication factor
3,dfs.client.mmap.cache.timeout.ms,3600000,"The minimum length of time that we will keep an mmap entry in the cache
    between uses.  If an entry is in the cache longer than this, and nobody
    uses it, it will be removed by a background thread."
4,dfs.datanode.du.reserved.pct,0,"Reserved space in percentage. Read dfs.datanode.du.reserved.calculator to see
    when this takes effect. The actual number of bytes reserved will be calculated by using the
    total capacity of the data directory in question. Specific storage type based reservation
    is also supported. The property can be followed with corresponding storage types
    ([ssd]/[disk]/[archive]/[ram_disk]) for cluster with heterogeneous storage.
    For example, reserved percentage space for RAM_DISK storage can be configured using property
    'dfs.datanode.du.reserved.pct.ram_disk'. If specific storage type reservation is not configured
    then dfs.datanode.du.reserved.pct will be used."
5,dfs.client.socketcache.expiryMsec,3000,Socket cache expiration for short-circuit reads in msec.
6,dfs.namenode.checkpoint.period,3600,"The number of seconds between two periodic checkpoints.
    Support multiple time unit suffix(case insensitive), as described
    in dfs.heartbeat.interval.If no time unit is specified then seconds
    is assumed."
7,dfs.client.block.write.replace-datanode-on-failure.min-replication,0,"The minimum number of replications that are needed to not to fail
      the write pipeline if new datanodes can not be found to replace
      failed datanodes (could be due to network failure) in the write pipeline.
      If the number of the remaining datanodes in the write pipeline is greater
      than or equal to this property value, continue writing to the remaining nodes.
      Otherwise throw exception.

      If this is set to 0, an exception will be thrown, when a replacement
      can not be found.
      See also dfs.client.block.write.replace-datanode-on-failure.policy"
8,fs.s3a.executor.capacity,16,"The maximum number of submitted tasks which is a single
    operation (e.g. rename(), delete()) may submit simultaneously for
    execution -excluding the IO-heavy block uploads, whose capacity
    is set in ""fs.s3a.fast.upload.active.blocks""

    All tasks are submitted to the shared thread pool whose size is
    set in ""fs.s3a.threads.max""; the value of capacity should be less than that
    of the thread pool itself, as the goal is to stop a single operation
    from overloading that thread pool."
