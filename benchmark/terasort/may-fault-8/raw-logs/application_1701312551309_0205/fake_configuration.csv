,name,value,description
0,io.bytes.per.checksum,0,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.interval-seconds,60,
2,yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size,10000,
3,mapreduce.reduce.maxattempts,4,"Expert: The maximum number of attempts per reduce task.
  In other words, framework will try to execute a reduce task these many number
  of times before giving up on it."
4,hadoop.security.dns.log-slow-lookups.threshold.ms,1000,"If slow lookup logging is enabled, this threshold is used to decide if a
    lookup is considered slow enough to be logged."
5,yarn.nodemanager.logaggregation.threadpool-size-max,100,
6,dfs.balancer.moverThreads,1000,"Thread pool size for executing block moves.
    moverThreadAllocator"
7,dfs.client.failover.sleep.base.millis,500,"Expert only. The time to wait, in milliseconds, between failover
    attempts increases exponentially as a function of the number of
    attempts made so far, with a random factor of +/- 50%. This option
    specifies the base value used in the failover calculation. The
    first failover will retry immediately. The 2nd failover attempt
    will delay at least dfs.client.failover.sleep.base.millis
    milliseconds. And so on."
8,fs.s3a.max.total.tasks,32,"The number of operations which can be queued for execution.
  This is in addition to the number of active threads in fs.s3a.threads.max."
9,dfs.namenode.max.op.size,52428800,Maximum opcode size in bytes.
