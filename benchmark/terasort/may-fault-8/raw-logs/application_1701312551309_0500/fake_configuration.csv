,name,value,description
0,io.bytes.per.checksum,0,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,yarn.sharedcache.cleaner.period-mins,1440,
2,dfs.client.block.write.retries,3,"The number of retries for writing blocks to the data nodes, 
  before we signal failure to the application."
3,mapreduce.input.fileinputformat.split.minsize,0,"The minimum size chunk that map input should be split
  into.  Note that some file formats may have minimum split sizes that
  take priority over this setting."
4,dfs.client.block.write.locateFollowingBlock.max.delay.ms,60000,The maximum delay (unit is ms) before retrying locateFollowingBlock.
5,dfs.journalnode.sync.interval,120000,"Time interval, in milliseconds, between two Journal Node syncs.
    This configuration takes effect only if the journalnode sync is enabled
    by setting the configuration parameter dfs.journalnode.enable.sync to true."
6,dfs.client.failover.connection.retries.on.timeouts,0,"Expert only. The number of retry attempts a failover IPC client
    will make on socket timeout when establishing a server connection."
7,mapreduce.job.running.map.limit,0,"The maximum number of simultaneous map tasks per job.
  There is no limit if this value is 0 or negative."
8,dfs.client.hedged.read.threshold.millis,500,"Configure 'hedged' reads in DFSClient. This is the number of milliseconds
    to wait before starting up a 'hedged' read."
9,mapreduce.map.skip.maxrecords,0,"The number of acceptable skip records surrounding the bad
    record PER bad record in mapper. The number includes the bad record as well.
    To turn the feature of detection/skipping of bad records off, set the
    value to 0.
    The framework tries to narrow down the skipped range by retrying
    until this threshold is met OR all attempts get exhausted for this task.
    Set the value to Long.MAX_VALUE to indicate that framework need not try to
    narrow down. Whatever records(depends on application) get skipped are
    acceptable."
