,name,value,description
0,io.bytes.per.checksum,-1834508200,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,mapreduce.reduce.shuffle.retry-delay.max.ms,60000,"The maximum number of ms the reducer will delay before retrying
  to download map data."
2,dfs.datanode.scan.period.hours,504,"If this is positive, the DataNode will not scan any
        individual block more than once in the specified scan period.
        If this is negative, the block scanner is disabled.
        If this is set to zero, then the default value of 504 hours
        or 3 weeks is used. Prior versions of HDFS incorrectly documented
        that setting this key to zero will disable the block scanner."
3,dfs.balancer.max-no-move-interval,60000,"If this specified amount of time has elapsed and no block has been moved
    out of a source DataNode, on more effort will be made to move blocks out of
    this DataNode in the current Balancer iteration."
4,ha.failover-controller.cli-check.rpc-timeout.ms,20000,"Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState"
5,dfs.block.invalidate.limit,1000,"The maximum number of invalidate blocks sent by namenode to a datanode
    per heartbeat deletion command. This property works with
    ""dfs.namenode.invalidate.work.pct.per.iteration"" to throttle block
    deletions."
6,yarn.resourcemanager.delegation.token.max-lifetime,604800000,
7,dfs.provided.aliasmap.load.retries,0,"The number of retries on the Datanode to load the provided aliasmap;
      defaults to 0."
8,yarn.resourcemanager.nm-container-queuing.min-queue-length,5,
9,io.file.buffer.size,4096,"The size of buffer for use in sequence files.
  The size of this buffer should probably be a multiple of hardware
  page size (4096 on Intel x86), and it determines how much data is
  buffered during read and write operations."
