,name,value,description
0,io.bytes.per.checksum,-229054672,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,dfs.client.block.write.replace-datanode-on-failure.min-replication,0,"The minimum number of replications that are needed to not to fail
      the write pipeline if new datanodes can not be found to replace
      failed datanodes (could be due to network failure) in the write pipeline.
      If the number of the remaining datanodes in the write pipeline is greater
      than or equal to this property value, continue writing to the remaining nodes.
      Otherwise throw exception.

      If this is set to 0, an exception will be thrown, when a replacement
      can not be found.
      See also dfs.client.block.write.replace-datanode-on-failure.policy"
2,io.seqfile.compress.blocksize,1000000,"The minimum block size for compression in block compressed
          SequenceFiles."
3,dfs.namenode.decommission.interval,30,"Namenode periodicity in seconds to check if
    decommission or maintenance is complete. Support multiple time unit
    suffix(case insensitive), as described in dfs.heartbeat.interval.
    If no time unit is specified then seconds is assumed."
4,ha.health-monitor.rpc.connect.max.retries,1,"The number of retries on connect error when establishing RPC proxy
    connection to NameNode, used for monitorHealth() calls."
5,dfs.blockreport.split.threshold,1000000,"If the number of blocks on the DataNode is below this
    threshold then it will send block reports for all Storage Directories
    in a single message.

    If the number of blocks exceeds this threshold then the DataNode will
    send block reports for each Storage Directory in separate messages.

    Set to zero to always split."
6,dfs.client.read.shortcircuit.streams.cache.expiry.ms,300000,"This controls the minimum amount of time
    file descriptors need to sit in the client cache context
    before they can be closed for being inactive for too long."
7,dfs.datanode.metrics.logger.period.seconds,600,"This setting controls how frequently the DataNode logs its metrics. The
    logging configuration must also define one or more appenders for
    DataNodeMetricsLog for the metrics to be logged.
    DataNode metrics logging is disabled if this value is set to zero or
    less than zero."
8,yarn.nodemanager.health-checker.interval-ms,600000,
9,dfs.client.deadnode.detection.rpc.threads,20,The maximum number of threads to use for calling RPC call to recheck the liveness of dead node.
