,name,value,description
0,io.bytes.per.checksum,-126213015,"The number of bytes per checksum.  Must not be larger than
  io.file.buffer.size."
1,dfs.content-summary.limit,5000,"The maximum content summary counts allowed in one locking period. 0 or a negative number
    means no limit (i.e. no yielding)."
2,dfs.namenode.block.deletion.increment,1000,"The number of block deletion increment.
      This setting will control the block increment deletion rate to
      ensure that other waiters on the lock can get in."
3,dfs.edit.log.transfer.bandwidthPerSec,0,"Maximum bandwidth used for transferring edit log to between journal nodes
    for syncing, in bytes per second.
    A default value of 0 indicates that throttling is disabled."
4,hadoop.security.groups.negative-cache.secs,30,"Expiration time for entries in the the negative user-to-group mapping
    caching, in seconds. This is useful when invalid users are retrying
    frequently. It is suggested to set a small value for this expiration, since
    a transient error in group lookup could temporarily lock out a legitimate
    user.

    Set this to zero or negative value to disable negative user-to-group caching."
5,ipc.[port_number].decay-scheduler.period-ms,5000,"How frequently the decay factor should be applied to the
    operation counts of users. Higher values have less overhead, but respond
    less quickly to changes in client behavior.
    This property applies to DecayRpcScheduler."
6,mapreduce.reduce.maxattempts,4,"Expert: The maximum number of attempts per reduce task.
  In other words, framework will try to execute a reduce task these many number
  of times before giving up on it."
7,dfs.client.block.write.retries,3,"The number of retries for writing blocks to the data nodes, 
  before we signal failure to the application."
8,mapreduce.jobhistory.client.thread-count,10,The number of threads to handle client API requests
9,dfs.client.read.shortcircuit.buffer.size,1048576,Buffer size in bytes for short-circuit local reads.
