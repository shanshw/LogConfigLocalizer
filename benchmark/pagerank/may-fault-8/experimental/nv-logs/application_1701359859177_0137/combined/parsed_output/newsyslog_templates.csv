EventId,EventTemplate,Occurrences
19b75bb3,Created MRAppMaster for application appattempt_<*>_<*>_<*>,2
b6c60981,Updating Configuration,2
0cd5c477,"Executing with tokens: [Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: <*> cluster_timestamp: <*> } attemptId: <*> } keyId: <*>)]",2
70f56aad,resource-types.xml not found,2
860c743d,Unable to find 'resource-types.xml'.,2
c0c8618d,OutputCommitter set in config null,2
7495049a,OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter,2
3a2b3aef,Registering class <*> for class <*>,18
fb1f4268,Default file system [hdfs://2f08f873c798:<*>],6
3a03968c,Emitting job history data to the timeline server is not enabled,2
7a3d0110,Loaded properties from hadoop-metrics2.properties,2
becc4e71,Scheduled Metric snapshot period at <*> second(s).,2
fc657c96,MRAppMaster metrics system started,2
080bfa16,Adding job token for job_<*>_<*> to jobTokenSecretManager,2
79b8e446,Not uberizing job_<*>_<*> because: not enabled; too many maps; too many reduces; too much RAM;,2
7ee6a6bc,Input size for job job_<*>_<*> = <*>. Number of splits = <*>,2
691cf50f,Number of reduces for job job_<*>_<*> = <*>,2
a58e16d8,job_<*>_0137Job Transitioned from <*> to <*>,12
8cfccb68,"MRAppMaster launching normal, non-uberized, multi-container job job_<*>_<*>.",2
4800a8aa,"Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: <*>, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.",4
55039769,Listener at <*>,4
70066a26,Starting Socket Reader #<*> for port <*>,4
8c4d0a72,Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server,2
ccc17099,IPC Server Responder: starting,4
73b103d7,IPC Server listener on <*>: starting,4
3592ea3d,Instantiated MRClientService at cf6e5ca2712c<*>,2
9a74e8e5,Logging initialized @2519ms to org.eclipse.jetty.util.log.Slf4jLog,2
9c1a3d53,"Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /root/hadoop-http-auth-signature-secret",2
0859dbfb,Http request log for http.requests.mapreduce is not defined,2
a8927e0e,Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter),2
ae2086c0,Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context <*>,4
5ba9445a,Registered webapp guice modules,2
8aa20788,Jetty bound to port <*>,2
16a015cc,jetty-<*>.<*>.<*>.v20230217; built: <*>-<*>-17T08:<*>:<*>.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm <*>.<*>.<*>+<*>-post-Ubuntu-1ubuntu120.<*>,2
bacbae49,DefaultSessionIdManager workerName=node<*>,2
9ee7bbd8,"No SessionScavenger set, using defaults",2
0fcdc0c2,node0 Scavenging every 600000ms,2
00ac854a,Started <*>,6
dd35c99e,"Started ServerConnector@78fe204a{HTTP/<*>.<*>, (http/<*>.<*>)}{<*>}",2
0923c3bb,Web app mapreduce started at <*>,2
e79de774,JOB_CREATE job_<*>_<*>,2
8b1732f5,nodeBlacklistingEnabled:true,2
07a75a5d,maxTaskFailuresPerNode is <*>,2
168a0869,blacklistDisablePercent is <*>,2
10c1693b,0% of the mappers will be scheduled using OPPORTUNISTIC containers,2
4ecdc8c5,Connecting to ResourceManager at 2f08f873c798<*>,2
1b9a71a7,"maxContainerCapability: <memory:<*>, vCores:<*>>",2
62365dbe,queue: default,2
293bdc87,Upper limit on the thread pool size is <*>,2
7a27b63d,The thread pool initial size is <*>,2
16c6feb3,"Thread Thread[eventHandlingThread,<*>,main] threw an Exception.org.apache.hadoop.HadoopIllegalArgumentException: Invalid checksum type: userOpt=null, default=CRC32C:<*>, effective=null at org.apache.hadoop.hdfs.client.impl.DfsClientConf.createChecksum(DfsClientConf.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem$<*>.doCall(DistributedFileSystem.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem$<*>.doCall(DistributedFileSystem.java:<*>) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.createEventWriter(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.setupEventWriter(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$<*>.run(JobHistoryEventHandler.java:<*>) at java.base/java.lang.Thread.run(Thread.java:<*>)",2
8825a75c,Processing the event EventType: <*>,22
24f60e3d,File Output Committer Algorithm version is <*>,2
d1f83c9a,"FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false",2
16c3067a,"Resource capability of task type <*> is set to <memory:<*>, vCores:<*>>",4
d231a1af,task_<*>_<*>_m_<*> Task Transitioned from <*> to <*>,66
a55d23fc,task_<*>_<*>_r_<*> Task Transitioned from <*> to <*>,78
590bd861,attempt_<*>_<*>_m_<*>_<*> TaskAttempt Transitioned from <*> to <*>,110
500abf63,attempt_<*>_<*>_r_<*>_<*> TaskAttempt Transitioned from <*> to <*>,268
70768b95,"mapResourceRequest:<memory:<*>, vCores:<*>>",2
fa3c4543,"reduceResourceRequest:<memory:<*>, vCores:<*>>",2
f3cfcdcd,Before Scheduling: PendingReds:<*> ScheduledMaps:<*> ScheduledReds:<*> AssignedMaps:<*> AssignedReds:<*> CompletedMaps:<*> CompletedReds:<*> ContAlloc:<*> ContRel:<*> HostLocal:<*> RackLocal:<*>,32
6d3d97fc,"getResources() for application_<*>_<*>: ask=<*> release= <*> newContainers=<*> finishedContainers=<*> resourcelimit=<memory:<*>, vCores:<*>> knownNMs=<*>",36
e856e146,"Recalculating schedule, headroom=<memory:<*>, vCores:<*>>",54
1fbfd66f,Reduce slow start threshold not met. completedMapsForReduceSlowstart <*>,10
0f71bb1f,Got allocated containers <*>,34
99dd87cd,Assigned container container_<*>_<*>_<*>_<*> to <*>,80
b1bdc9c9,After Scheduling: PendingReds:<*> ScheduledMaps:<*> ScheduledReds:<*> AssignedMaps:<*> AssignedReds:<*> CompletedMaps:<*> CompletedReds:<*> ContAlloc:<*> ContRel:<*> HostLocal:<*> RackLocal:<*>,34
c9ebd122,The <*> file on the remote FS is <*>,4
5ad17131,Adding #<*> tokens and #<*> secret keys for NM use for launching container,2
e497a05e,Size of containertokens_dob is <*>,2
944f8887,Putting shuffle token in serviceData,2
36fc78d1,Processing the event EventType: <*> for container container_<*>_<*>_<*>_<*> taskAttempt <*>,154
df347da1,Launching <*>,80
f4ac93f2,Shuffle port returned by ContainerManager for <*> : <*>,80
97232dd5,TaskAttempt: <*> using containerId: [container_<*>_<*>_<*>_<*> on NM: <*>,80
e215e532,ATTEMPT_START <*>,80
d8c9fc3d,Auth successful for job_<*>_<*> (auth:SIMPLE) from <*> / <*>,80
60675cee,JVM with ID : <*> asked for a task,80
2a1c144a,JVM with ID: <*> given task: <*>,80
b53b8ef4,Progress of TaskAttempt <*> is : <*>.<*>,102
d1df58d8,Done acknowledgment from attempt_<*>_<*>_m_<*>_<*>,22
1aac72f8,Task succeeded with attempt attempt_<*>_<*>_m_<*>_<*>,22
3a2741bf,Num completed Tasks: <*>,24
279694a4,Received completed container container_<*>_<*>_<*>_<*>,74
f7558028,Diagnostics report from <*>,74
ec6b84b4,Reduce slow start threshold reached. Scheduling reduces.,2
1885a18a,"completedMapPercent <*>.<*> totalResourceLimit:<memory:<*>, vCores:<*>> finalMapResourceLimit:<memory:<*>, vCores:<*>> finalReduceResourceLimit:<memory:<*>, vCores:<*>> netScheduledMapResource:<memory:<*>, vCores:<*>> netScheduledReduceResource:<memory:<*>, vCores:<*>>",24
abfa93dd,Ramping up <*>,2
ff65c706,Assigned to reduce,58
f13d7a4d,MapCompletionEvents request from attempt_<*>_<*>_r_<*>_<*>. startIndex <*> maxEvents <*>,84
983af3ff,All maps assigned. Ramping up all remaining reduces:<*>,20
682b8ffc,"Task: attempt_<*>_<*>_r_<*>_<*> - exited : org.apache.hadoop.HadoopIllegalArgumentException: Invalid checksum type: userOpt=null, default=CRC32C:<*>, effective=null at org.apache.hadoop.hdfs.client.impl.DfsClientConf.createChecksum(DfsClientConf.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem$<*>.doCall(DistributedFileSystem.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem$<*>.doCall(DistributedFileSystem.java:<*>) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:<*>) at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.<init>(ReduceTask.java:<*>) at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:<*>) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:<*>) at org.apache.hadoop.mapred.YarnChild$<*>.run(YarnChild.java:<*>) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:<*>) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<*>) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:<*>)",58
876cf830,"Diagnostics report from attempt_<*>_<*>_r_<*>_<*>: Error: org.apache.hadoop.HadoopIllegalArgumentException: Invalid checksum type: userOpt=null, default=CRC32C:<*>, effective=null at org.apache.hadoop.hdfs.client.impl.DfsClientConf.createChecksum(DfsClientConf.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem$<*>.doCall(DistributedFileSystem.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem$<*>.doCall(DistributedFileSystem.java:<*>) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:<*>) at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.<init>(ReduceTask.java:<*>) at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:<*>) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:<*>) at org.apache.hadoop.mapred.YarnChild$<*>.run(YarnChild.java:<*>) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:<*>) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<*>) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:<*>)",116
0e4be9d6,<*> failures on node <*>,18
b2abfdcd,"attempt_<*>_<*>_r_<*>_<*> transitioned from state FAIL_FINISHING_CONTAINER to FAILED, event type is <*> and <*>",58
a0d2d61b,Blacklisted host <*>,6
20ca077e,Update the blacklist for application_<*>_<*>: blacklistAdditions=<*> blacklistRemovals=<*>,4
350cb79a,"Ignore blacklisting set to true. Known: <*>, Blacklisted: <*>, <*>%",2
db25a59d,Job failed as tasks failed. failedMaps:<*> failedReduces:<*> killedMaps:<*> killedReduces: <*>,2
0ff891cc,Could not deallocate container for task attemptId attempt_<*>_<*>_r_<*>_<*>,4
ad1a35c9,"Job finished cleanly, recording last MRAppMaster retry",2
5735d762,Notify <*> isAMLastRetry: true,4
b2ea851d,RMCommunicator notified that shouldUnregistered is: true,2
b288d94f,JobHistoryEventHandler notified that forceJobCompletion is true,2
2784eb31,Calling stop for all the services,2
c02382a8,Stopping JobHistoryEventHandler. Size of the outstanding queue size is <*>,2
6e5e38d1,"In stop, writing event JOB_SUBMITTED",2
22664475,Service <*> failed in state STOPPEDjava.lang.NullPointerException at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:<*>) at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:<*>) at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:<*>) at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:<*>) at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:<*>) at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler$<*>.run(MRAppMaster.java:<*>),4
79523fbd,When stopping the service JobHistoryEventHandlerjava.lang.NullPointerException at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:<*>) at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:<*>) at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:<*>) at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:<*>) at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:<*>) at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler$<*>.run(MRAppMaster.java:<*>),2
c0ef3cfb,KILLING attempt_<*>_<*>_r_<*>_<*>,6
ca66a1c8,Setting job diagnostics to Task failed task_<*>_<*>_r_000001Job failed as tasks failed. failedMaps:<*> failedReduces:<*> killedMaps:<*> killedReduces: <*>,2
03ca4927,History url is null,2
fd178178,Waiting for application to be successfully unregistered.,2
f88b4e72,Final Stats: PendingReds:<*> ScheduledMaps:<*> ScheduledReds:<*> AssignedMaps:<*> AssignedReds:<*> CompletedMaps:<*> CompletedReds:<*> ContAlloc:<*> ContRel:<*> HostLocal:<*> RackLocal:<*>,2
3b7f8573,Deleting staging directory hdfs://2f08f873c798:<*> /tmp/hadoop-yarn/staging/root/.staging/job_<*>_<*>,2
fa5d86c2,Stopping server on <*>,2
da6217eb,Stopping IPC Server listener on <*>,2
8429e6cc,Stopping IPC Server Responder,2
f495a4dd,TaskHeartbeatHandler thread interrupted,2
4594e7ea,TaskAttemptFinishingMonitor thread interrupted,2
9ab47de7,Graceful stop failed. Exiting..java.lang.NullPointerException at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:<*>) at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:<*>) at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:<*>) at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:<*>) at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:<*>) at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:<*>) at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler$<*>.run(MRAppMaster.java:<*>),2
61bb61d6,Exiting with status <*>: java.lang.NullPointerException,2
