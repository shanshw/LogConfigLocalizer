,name,value,description
0,io.bytes.per.checksum,-1202633397,
1,dfs.balancer.getBlocks.min-block-size,10485760,"Minimum block threshold size in bytes to ignore when fetching a source's
    block list."
2,yarn.scheduler.maximum-allocation-vcores,4,
3,mapreduce.reduce.shuffle.read.timeout,180000,"Expert: The maximum amount of time (in milli seconds) reduce
  task waits for map output data to be available for reading after obtaining
  connection."
4,hadoop.registry.zk.retry.interval.ms,1000,
5,mapreduce.job.cache.limit.max-resources,0,"The maximum number of resources a map reduce job is allowed to
    submit for localization via files, libjars, archives, and jobjar command
    line arguments and through the distributed cache. If set to 0 the limit is
    ignored."
6,yarn.nodemanager.runtime.linux.docker.stop.grace-period,10,
7,dfs.namenode.edit.log.autoroll.check.interval.ms,300000,"How often an active namenode will check if it needs to roll its edit log,
    in milliseconds."
8,seq.io.sort.mb,100,"The total amount of buffer memory to use while sorting files,
      while using SequenceFile.Sorter, in megabytes. By default,
      gives each merge stream 1MB, which should minimize seeks."
9,dfs.datanode.ec.reconstruction.stripedread.buffer.size,65536,Datanode striped read buffer size.
