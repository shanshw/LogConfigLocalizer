EventId,EventTemplate,Occurrences
19b75bb3,Created MRAppMaster for application appattempt_<*>_<*>_<*>,2
b6c60981,Updating Configuration,2
0cd5c477,"Executing with tokens: [Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: <*> cluster_timestamp: <*> } attemptId: <*> } keyId: <*>)]",2
e22de8d1,found resource resource-types.xml at file:/usr/local/revisedJQF/v8/hadoop-<*>.<*>.<*>/etc/hadoop/resource-types.xml,2
c0c8618d,OutputCommitter set in config null,2
7495049a,OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter,2
3a2b3aef,Registering class <*> for class <*>,18
fb1f4268,Default file system [hdfs://2f08f873c798:<*>],6
3a03968c,Emitting job history data to the timeline server is not enabled,2
7a3d0110,Loaded properties from hadoop-metrics2.properties,2
becc4e71,Scheduled Metric snapshot period at <*> second(s).,2
fc657c96,MRAppMaster metrics system started,2
080bfa16,Adding job token for job_<*>_<*> to jobTokenSecretManager,2
79b8e446,Not uberizing job_<*>_<*> because: not enabled; too many maps; too many reduces; too much RAM;,2
7ee6a6bc,Input size for job job_<*>_<*> = <*>. Number of splits = <*>,2
691cf50f,Number of reduces for job job_<*>_<*> = <*>,2
5e87a19b,job_<*>_0260Job Transitioned from <*> to <*>,4
8cfccb68,"MRAppMaster launching normal, non-uberized, multi-container job job_<*>_<*>.",2
4800a8aa,"Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: <*>, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.",4
55039769,Listener at <*>,4
70066a26,Starting Socket Reader #<*> for port <*>,4
8c4d0a72,Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server,2
ccc17099,IPC Server Responder: starting,4
73b103d7,IPC Server listener on <*>: starting,4
a45cc31c,Instantiated MRClientService at 2f08f873c798<*>,2
079885f8,Logging initialized @2235ms to org.eclipse.jetty.util.log.Slf4jLog,2
9c1a3d53,"Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /root/hadoop-http-auth-signature-secret",2
0859dbfb,Http request log for http.requests.mapreduce is not defined,2
a8927e0e,Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter),2
ae2086c0,Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context <*>,4
5ba9445a,Registered webapp guice modules,2
8aa20788,Jetty bound to port <*>,2
cd096556,jetty-<*>.<*>.<*>.v20230217; built: <*>-<*>-17T08:<*>:<*>.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm <*>.<*>.<*>+<*>-post-Ubuntu-0ubuntu120.<*>,2
bacbae49,DefaultSessionIdManager workerName=node<*>,2
9ee7bbd8,"No SessionScavenger set, using defaults",2
0fcdc0c2,node0 Scavenging every 600000ms,2
00ac854a,Started <*>,6
2841411b,"Started ServerConnector@71a3e05c{HTTP/<*>.<*>, (http/<*>.<*>)}{<*>}",2
0923c3bb,Web app mapreduce started at <*>,2
e79de774,JOB_CREATE job_<*>_<*>,2
8b1732f5,nodeBlacklistingEnabled:true,2
07a75a5d,maxTaskFailuresPerNode is <*>,2
168a0869,blacklistDisablePercent is <*>,2
10c1693b,0% of the mappers will be scheduled using OPPORTUNISTIC containers,2
4ecdc8c5,Connecting to ResourceManager at 2f08f873c798<*>,2
1b9a71a7,"maxContainerCapability: <memory:<*>, vCores:<*>>",2
62365dbe,queue: default,2
293bdc87,Upper limit on the thread pool size is <*>,2
7a27b63d,The thread pool initial size is <*>,2
33c8c04a,Processing the event EventType: JOB_SETUP,2
24f60e3d,File Output Committer Algorithm version is <*>,2
d1f83c9a,"FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false",2
cdcb111a,"Thread Thread[eventHandlingThread,<*>,main] threw an Error. Shutting down now...java.lang.OutOfMemoryError: Java heap space at org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:<*>) at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:<*>) at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:<*>) at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem$<*>.doCall(DistributedFileSystem.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem$<*>.doCall(DistributedFileSystem.java:<*>) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<*>) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.createEventWriter(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.setupEventWriter(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:<*>) at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$<*>.run(JobHistoryEventHandler.java:<*>) at java.base/java.lang.Thread.run(Thread.java:<*>)",2
023beecf,Halt with status <*>: HaltException-<*>: HaltException at org.apache.hadoop.util.ExitUtil.halt(ExitUtil.java:<*>) at org.apache.hadoop.util.ExitUtil.halt(ExitUtil.java:<*>) at org.apache.hadoop.yarn.YarnUncaughtExceptionHandler.uncaughtException(YarnUncaughtExceptionHandler.java:<*>) at java.base/java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:<*>) at java.base/java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:<*>) at java.base/java.lang.Thread.dispatchUncaughtException(Thread.java:<*>),1
9b74d63c,Halt with status <*>: HaltException,1
